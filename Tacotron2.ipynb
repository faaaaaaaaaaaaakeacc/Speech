{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tacotron2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajUbY8p0gkeL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import ipywidgets as widgets\n",
        "import itertools\n",
        "from torch import optim\n",
        "from torchaudio.transforms import RNNTLoss\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "from IPython.display import display, clear_output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir(\"./data\"):\n",
        "    os.makedirs(\"./data\")\n",
        "\n",
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)\n"
      ],
      "metadata": {
        "id": "ud6CYWNujKYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "iFDnRzNhkbZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MFCC(sample_rate=16000, n_mfcc=128)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "qF7LMU1_k5J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, device):\n",
        "        self.word2ind = {}\n",
        "        self.ind2word = {}\n",
        "        self.num_words = 0\n",
        "        self.device = device\n",
        "        self._add_word(\"<UNK>\")\n",
        "\n",
        "\n",
        "    def _add_word(self, word):\n",
        "        if word not in self.word2ind.keys():\n",
        "            self.word2ind[word] = self.num_words + 1\n",
        "            self.ind2word[self.num_words + 1] = word\n",
        "            self.num_words += 1\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in tokenize(sentence):\n",
        "            self._add_word(word)\n",
        "\n",
        "    def tokenize_sentence(self, sentence):\n",
        "        result_list = []\n",
        "        for word in tokenize(sentence):\n",
        "            if word in self.word2ind.keys():\n",
        "                result_list.append(self.word2ind[word])\n",
        "            else:\n",
        "                result_list.append(self.word2ind[\"<UNK>\"])\n",
        "        return torch.LongTensor(result_list).to(device) \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_words + 10\n"
      ],
      "metadata": {
        "id": "j2tKekzmk52K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "vocab = Vocab(device)\n",
        "\n",
        "for batch in tqdm(train_dataset):\n",
        "    vocab.add_sentence(batch[2])\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "1YdNV0vEqISB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "embeddings = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "Zp6ZiqoFr_TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'hello' in embeddings.vocab"
      ],
      "metadata": {
        "id": "HT5e8hgI7bBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from tqdm.auto import trange\n",
        "\n",
        "class TacotronEncoder(nn.Module):\n",
        "    def __init__(self): \n",
        "        super().__init__()\n",
        "        weights = []\n",
        "        for i in trange(len(vocab)):\n",
        "            if i in vocab.ind2word.keys() and vocab.ind2word[i] in embeddings.vocab:\n",
        "                weights.append(list(embeddings.get_vector(vocab.ind2word[i])))\n",
        "            else:\n",
        "                weights.append(list(np.zeros(300)))\n",
        "\n",
        "        weights = torch.FloatTensor(weights).to(device)\n",
        "        self.embedding = nn.Embedding.from_pretrained(weights)   \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 1, 3, 1, 1)\n",
        "        )\n",
        "        self.rnn = nn.LSTM(300, 64, 1, bidirectional = True, batch_first=True)\n",
        "\n",
        "    def forward(self, input_text):\n",
        "        # 1, L\n",
        "        hidden = self.embedding(input_text)\n",
        "        # 1, L, 300\n",
        "        hidden = hidden.unsqueeze(0)\n",
        "        # 1, 1, L, 300\n",
        "        hidden = self.conv(hidden)\n",
        "        # 1, 1, L, 300\n",
        "        hidden = hidden.squeeze(0)\n",
        "        # 1, L, 300\n",
        "        hidden, (h, c) = self.rnn(hidden)\n",
        "        # 1, L, 128\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "zXt8qIwbqQqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_sanity_check():\n",
        "    encoder = TacotronEncoder().to(device)\n",
        "    text = train_dataset[0][2]\n",
        "    tensor_text = vocab.tokenize_sentence(text).unsqueeze(0)\n",
        "    encoded = encoder(tensor_text)\n",
        "    print(encoded.shape)\n",
        "\n",
        "encoder_sanity_check()"
      ],
      "metadata": {
        "id": "ZNi4k8_56pjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNWithAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(128, 64, 1, bidirectional = True, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 32),\n",
        "            nn.LeakyReLU(0.05),\n",
        "            nn.Linear(32, 128)\n",
        "        )\n",
        "        self.fc2 = torch.ones(1, 128).to(device)\n",
        "\n",
        "    def forward(self, input_seq, encoder_output):\n",
        "        # 1, L, embedding_dim | 1, T, 128\n",
        "        h, c = None, None\n",
        "        result_vecs = None\n",
        "        for i in range(input_seq.shape[1]):\n",
        "            cur_vector = input_seq[0, i, :]\n",
        "            # embedding_dim\n",
        "            vec2attn = self.fc(cur_vector).unsqueeze(1)\n",
        "            # 128, 1\n",
        "            weights = (encoder_output @ vec2attn)\n",
        "            # 1, T, 1\n",
        "            weights_norm = weights @ self.fc2\n",
        "            # 1, T, 128\n",
        "            sum_outputs = encoder_output * encoder_output \n",
        "            result_attention_vec = torch.sum(sum_outputs, 1).unsqueeze(0)\n",
        "            # 1, 1, 128\n",
        "            if h is None:\n",
        "                result_vecs, (h, c) = self.rnn(result_attention_vec)\n",
        "            else:\n",
        "                rnn_output, (h, c) = self.rnn(result_attention_vec,  (h, c))\n",
        "                result_vecs = torch.cat((result_vecs, rnn_output), 1)\n",
        "\n",
        "        # 1, L, 128 \n",
        "        return result_vecs\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "PU1dV9557UVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_sanity_check():\n",
        "    encoder = RNNWithAttention(128).to(device)\n",
        "    encoded = encoder(torch.zeros((1, 33, 128)).to(device), torch.zeros((1, 23, 128)).to(device))\n",
        "    print(encoded.shape)\n",
        "\n",
        "attention_sanity_check()"
      ],
      "metadata": {
        "id": "LJr6g2fWForS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TacotronDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn1 = RNNWithAttention(128)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.rnn2 = RNNWithAttention(128)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 1, 3, 1, 1)\n",
        "        )\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "\n",
        "    def forward(self, input_spectragram, encoder_output):\n",
        "        hidden = self.rnn1(input_spectragram, encoder_output)\n",
        "        hidden1 = self.fc1(hidden)\n",
        "        hidden2 = self.rnn2(hidden1, encoder_output)\n",
        "        hidden3 = self.fc2(hidden) + hidden2\n",
        "        return self.conv1(hidden3.unsqueeze(1)).squeeze(1)"
      ],
      "metadata": {
        "id": "cKSJGMrlH3Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_sanity_check():\n",
        "    encoder = TacotronDecoder().to(device)\n",
        "    encoded = encoder(torch.zeros((1, 33, 128)).to(device), torch.zeros((1, 23, 128)).to(device))\n",
        "    print(encoded.shape)\n",
        "decoder_sanity_check()"
      ],
      "metadata": {
        "id": "j_jkcLDqX70q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tacotron(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = TacotronEncoder()\n",
        "        self.decoder = TacotronDecoder()\n",
        "    \n",
        "    def forward(self, input_spec, input_text):\n",
        "        decoded_text = self.encoder(input_text)\n",
        "        encoded_spec = self.decoder(input_spec, decoded_text)\n",
        "        return encoded_spec"
      ],
      "metadata": {
        "id": "FqMKLfWVYleN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_spectrogram(t2s_model: Tacotron, input_text, start_token, end_token, num_iterations, eps):\n",
        "    encoded_text = t2s_model.encoder(input_text)\n",
        "    answer = start_token\n",
        "    for i in range(num_iterations):\n",
        "        output_decoder = t2s_model(answer)\n",
        "        last_token = output_decoder[:, -1, :].reshape(1, 1, -1)\n",
        "        if torch.sum(end_token - last_token).item() < eps:\n",
        "            break\n",
        "        answer = torch.cat((answer, last_token), 1)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "9-qBjM4lcoa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN = torch.zeros((1, 1, 128)).to(device)\n",
        "END_TOKEN = torch.zeros((1, 1, 128)).to(device)"
      ],
      "metadata": {
        "id": "y6taDSvHe92k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Tacotron().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)"
      ],
      "metadata": {
        "id": "tNSDPynufG02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import trange, tqdm\n",
        "import queue\n",
        "\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in trange(num_epochs):\n",
        "    pbar = tqdm(train_dataset)\n",
        "    sum_loss, cnt_loss = 0, 0\n",
        "    log_window = 10\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        input_text = batch[2]\n",
        "        tensor_text = vocab.tokenize_sentence(input_text).reshape(1, -1)\n",
        "        input_wav = batch[0].to(device)\n",
        "        input_spectrogram = dataset_transforms(input_wav).permute(0, 2, 1)\n",
        "        model_input = torch.cat((START_TOKEN, input_spectrogram), 1)\n",
        "        model_target = torch.cat((input_spectrogram, END_TOKEN), 1)\n",
        "        model_output = model(model_input, tensor_text)\n",
        "        loss = criterion(model_output, model_target)\n",
        "        loss.backward()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "        cnt_loss += 1\n",
        "\n",
        "        descritption = f\"Last loss : {round(loss.item(), 2)} | Mean loss : {round(sum_loss/cnt_loss, 2)}\"\n",
        "        pbar.set_description(descritption)\n",
        "        if cnt_loss == log_window:\n",
        "            sum_loss, cnt_loss = 0, 0\n",
        "        optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "jbrI7mxEffb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from librosa.feature.inverse import mfcc_to_audio\n",
        "\n",
        "input_wav = batch[0].to(device)\n",
        "sample_spec = dataset_transforms(input_wav)\n",
        "mfcc_to_audio(sample_spec.cpu().detach().numpy().squeeze(0))\n"
      ],
      "metadata": {
        "id": "YFf19nlSik46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dy7qYol9nX2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}